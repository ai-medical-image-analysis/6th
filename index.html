<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>PHAROS-AIF-MIH Workshop</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="img/favicon.png" rel="icon">
  <link href="img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="vendor/aos/aos.css" rel="stylesheet">
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Gp
  * Updated: Nov 25 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/gp-free-multipurpose-html-bootstrap-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top ">
    <div class="container d-flex align-items-center justify-content-lg-between">

      <h1 class="logo me-auto me-lg-0"><a href="index.html">PHAROS-AIF-MIH</a></h1>
      <!-- Uncomment below if you prefer to use an image logo -->
      <!-- <a href="index.html" class="logo me-auto me-lg-0"><img src="img/logo.png" alt="" class="img-fluid"></a>-->

      <nav id="navbar" class="navbar order-last order-lg-0">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="#about">About</a></li>
          <li><a class="nav-link scrollto" href="#team">Organisers</a></li>
          <li><a class="nav-link scrollto" href="#portfolio">Workshop</a></li>
 
          
          <li class="dropdown"><a href="#clients"><span>Competition</span> <i class="bi bi-chevron-down"></i></a>
            <ul>
              <li><a href="#features">Multi-Source Covid-19 Detection</a></li>
              <li><a href="#features2">Fair Disease Diagnosis</a></li>
              
        
          </li>

        </ul>
      </nav><!-- .navbar -->

       <a href="https://cmt3.research.microsoft.com/PHAROS2026" class="get-started-btn scrollto">Submission Site</a> 

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero" class="d-flex align-items-center justify-content-center">
    <div class="container" data-aos="fade-up">

          
      <br> <h1> PHAROS AI Factory   </h1> 
      <br> <h1> for Medical Imaging & Healthcare  </h1> 
      <br> <h1> (PHAROS-AIF-MIH)</h1>
      
      <div class="row justify-content-center" data-aos="fade-up" data-aos-delay="150">
        <div class="col-xl-6 col-lg-8">
         
          <h2>in conjunction with the IEEE Computer Vision and Pattern Recognition Conference (CVPR), 2026 </h2>
          <h2>  Wed June 3 - Sun June 7, 2026, Denver Colorado </h2>
        </div>
      </div>











    </div>
  </section><!-- End Hero -->






<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->


  <main id="main">

    <!-- ======= About Section ======= -->
    <section id="about" class="about">
      <div class="container" data-aos="fade-up">

          <div class="col-lg-6 order-1 order-lg-2" data-aos="fade-right" data-aos-delay="100">
            <h3>About PHAROS-AIF-MIH</h3>
          </div>  

          <div class="text">

            <p>
            The PHAROS-AIF-MIH Workshop (linked to the healthcare vertical of PHAROS AI Factory) aims to foster discussion and presentation of ideas regarding challenges of analysis of CT scans, MRIs, X-rays, EHRs and content metadata in the context of Digital Pathology and Radiology, adopting recent advances in GenAI, multimodal large language models, privacy preserving architectures, ethics by design, AI models trained in supercomputing infrastructures, knowledge distillation and Agentic AI. A Competition will be also organized focusing on multi-disease diagnosis through multiple medical image datasets. Baseline models will be provided to participants who will develop AI models improving performance and fairness of results.
The topics covered by the PHAROS-AIF-MIH Workshop are closely related to the focus of the healthcare thematic area (vertical) of the PHAROS AI Factory, one of the first seven AI Factories selected and funded by the European Union in late 2024 (currently EU, through EuroHPC-JU, has selected 19 AI Factories, operating all over Europe). 
            </p>

           <p>  A Competition is also organized, which is split into two Challenges 
             (the Multi-Source Covid-19 Detection Challenge and the Fair Disease Diagnosis Challenge)
             offering two large databases and letting research groups test and
compare their state-of-the-art AI approaches.
             </p>



            
            <p>
            The PHAROS-AIF-MIH Workshop is the sixth in the AI-MIA series of Workshops  held at
<a href="https://ai-medical-image-analysis.github.io/5th/">ICCV 2025</a> and
              <a href="https://ai-medical-image-analysis.github.io/4th/">CVPR 2024</a> and
<a href="https://mlearn.lincoln.ac.uk/icassp-2023-ai-mia/">IEEE ICASSP 2023</a>, 
<a href="https://vcmi.inesctec.pt/aimia_eccv/">ECCV 2022</a> and
<a href="https://mlearn.lincoln.ac.uk/mia-cov19d/">ICCV 2021</a>
Conferences.  
            </p>

For any requests or enquiries regarding the Workshop, please contact: <a href="stefanos@cs.ntua.gr">stefanos@cs.ntua.gr</a>.
            
        </div>

      </div>
    </section><!-- End About Section -->






<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->

    <!-- ======= Team Section ======= -->
    <section id="team" class="team">
      <div class="container" data-aos="fade-up">

        <div class="d-flex align-items-center justify-content-center" data-aos="fade-right" data-aos-delay="100">
          <h2>Organisers</h2>
          
        </div>

<br></br>
        
        <div class="d-flex align-items-center justify-content-center" data-aos="fade-right" data-aos-delay="100">
          <h3>General Chair</h3>
          

                <br></br>

           </div>

         
        <div class="d-flex align-items-center justify-content-center" data-aos="fade-right" data-aos-delay="100">
          


          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up" data-aos-delay="100">
              <div class="member-img">
                <img src="img/stefanos.jpg" class="img-fluid" width="150" height="150" alt="">
                <div class="social">
                  <a href="http://www.image.ntua.gr/"><i class="bi bi-list"></i></a>
                  <a href="https://scholar.google.gr/citations?user=-h2va3cAAAAJ&hl=en"><i class="bi-journal-richtext"></i></a>
                </div>
              </div>
              <div class="member-info">
                <h4>Stefanos Kollias   </h4>
                <span>National Technical University </span>  <span>   of Athens & GRNET, Greece  </span>  

                <span>  stefanos@cs.ntua.gr </span>
              </div>
            </div>
          </div>

           </div>

     
       

    </div>

        <br></br>

         
        <div class="d-flex align-items-center justify-content-center" data-aos="fade-right" data-aos-delay="100">
          <h3>Program Chairs</h3>
          
        </div>
         
<br></br>

        
       <div class="row justify-content-center">

      <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up" data-aos-delay="100">
              <div class="member-img">
                <img src="img/jim.jpg" class="img-fluid" width="150" height="150" alt="">
                <div class="social">
                  <a href="https://sites.google.com/view/dimitrioskollias/home?authuser=0&pli=1"><i class="bi bi-list"></i></a>
                  <a href="https://scholar.google.com/citations?user=360Gmc0AAAAJ&hl=en"><i class="bi-journal-richtext"></i></a>
                </div>
              </div>
              <div class="member-info">
                <h4>Dimitrios Kollias   </h4>
                <span>Queen Mary University </span> <span> of London, UK </span> <span> d.kollias@qmul.ac.uk</span>
              </div>
            </div>
          </div>
         


         
   <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up" data-aos-delay="100">
              <div class="member-img">
                <img src="img/xujiong.jpg" class="img-fluid" width="150" height="150" alt="">
                <div class="social">
                  <a href="https://scholar.google.com/citations?user=UrhKCWwAAAAJ&hl=en"><i class="bi-journal-richtext"></i></a>
                </div>
              </div>
              <div class="member-info">
                <h4>Xujiong Ye  </h4>
                 <span>  University of Exeter, UK     </span> 
                 <span>   X.Ye2@exeter.ac.uk  </span>
              </div>
            </div>
          </div>

          <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up" data-aos-delay="100">
              <div class="member-img">
                <img src="img/fran.jpg" class="img-fluid" width="150" height="150" alt="">
                <div class="social">
                  <a href="https://scholar.google.com/citations?user=ouAcTAUAAAAJ&hl=en"><i class="bi-journal-richtext"></i></a>
                <a href="https://www.linkedin.com/in/alan-cowen" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                </div>
              </div>
              <div class="member-info">
                <h4>Francesco Rundo    </h4>
                <span> University of Catania, Italy     </span>
                <span>   francesco.rundo@unict.it   </span>
              </div>
            </div>
       </div>     
 </section><!-- End Team Section -->
        
          <h2> &nbsp &nbsp &nbsp &nbsp Data Chairs</h2>
          

<div class="text">
   
<p>
&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  Anastasios Arsenos, &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp National and Kapodistrian University of Athens   <br>
&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  Paraskevi Theofilou, &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp National Technical University of Athens    <br>
&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp   Manos Seferis, &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp   National Technical University of Athens    <br>

</p>


         </div>  
        
   



         

    
   





<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->




    <!-- ======= Portfolio Section ======= -->
    <section id="portfolio" class="portfolio">
      <div class="container" data-aos="fade-up">


        <div class="d-flex align-items-center justify-content-center">
          <h1>The Workshop</h1>
        </div>

<br> </br>

        <div class="section-title">
          <h2>Call for Papers</h2>
          
        </div>

<div class="text">
   
<p>
Original high-quality contributions (in terms of databases, surveys, studies, foundation models, techniques and methodologies)
are solicited on -but are not limited to- the following topics and technologies: </p>


  
  <ul>
              <p><i class="ri-check-double-line"></i> predictive modeling and deep learning models for medical imaging</p>
              <p><i class="ri-check-double-line"></i>  transparent, human-centered integration of GenAI (e.g., LLMs, VLMs) in health services</p>
              <p><i class="ri-check-double-line"></i>  creating trust among citizens on the use of responsible AI for healthcare</p>
              <p><i class="ri-check-double-line"></i>  efficient and friendly use of services, respect of privacy - taking into consideration the AI Act
and Data Act</p>
              <p><i class="ri-check-double-line"></i>  learning methods to leverage knowledge from well-explored domains - gain insight in low data
availability cases - enhance prediction accuracy for underrepresented health conditions</p>
              <p><i class="ri-check-double-line"></i>  AutoML, allowing automated selection of hyperparameters, feature engineering</p>
              <p><i class="ri-check-double-line"></i>  AI models on multimodal data (e.g. imaging, sequencing, medical records, open datasets) </p>
              <p><i class="ri-check-double-line"></i>  domain-specific AI models (e.g. in breast cancer, Alzheimer’s disease) </p>
              <p><i class="ri-check-double-line"></i>   AI prediction models for disease progression (e.g., survival ML for Electronic Health Records) </p>
              <p><i class="ri-check-double-line"></i> segmentation, classification, fair & explainable medical decision making </p>
              <p><i class="ri-check-double-line"></i> interpretable and actionable insights into AI-driven decisions </p>
              <p><i class="ri-check-double-line"></i> integration of uncertainty quantification mechanisms (e.g., prediction intervals, variance heatmaps)
- Bayesian methods, Monte Carlo dropout, ensemble modeling </p>
              <p><i class="ri-check-double-line"></i> drift monitoring and out-of-distribution detection, continual learning and model transportability - domain adaptation for timely model updates and refinements </p>
                           <p><i class="ri-check-double-line"></i> Agentic AI for medical imaging & healthcare </p>
                           <p><i class="ri-check-double-line"></i> High Performance Computing and AI for healthcare </p>

            
  </ul>
   

<br> </br>

  

        <div class="section-title">
          <h2>Workshop Important Dates </h2>
        </div>        


              <br>  <span> <em>Paper Submission Deadline: </em>    &nbsp &nbsp &nbsp   &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 23:59:59 AoE (Anywhere on Earth) March 18, 2026  </span> </br>
               <br>  <span>  <em>Review decisions sent to authors; Notification of acceptance: </em>  &nbsp &nbsp &nbsp  April 6, 2026 </span> </br> 
              <br>   <span>   <em>Camera ready version: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp April 10, 2026  </span> </br>
            
<br></br>
<br></br>
        <div class="section-title">
          <h2>Submission Information</h2>
          
        </div>        
              

<p>The paper format should adhere to the paper submission guidelines for main CVPR 2026 proceedings style. Please have a look at the Submission Guidelines Section <a href="https://cvpr.thecvf.com/Conferences/2025/AuthorGuidelines">here</a>. </p>

<p>We welcome full long paper submissions (between 5 and 8 pages, excluding references or supplementary materials). All submissions must be anonymous and conform to the CVPR 2026 standards for double-blind review. </p>

<p>All papers should be submitted using <a href="https://cmt3.research.microsoft.com/PHAROS2026">this CMT website</a><sup><a href="#footnote1">*</a></sup>. </p>  

<p>All accepted manuscripts will be part of CVPR 2026 conference proceedings. </p>

<br>
<!-- Footnote section -->
<p id="footnote1"><strong>*</strong> The Microsoft CMT service was used for managing the peer-reviewing process for this workshop. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.</p>



            </div>
          </div>


    </section><!-- End Portfolio Section -->


<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->




   




<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->




<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->




    <!-- ======= Clients Section ======= -->
    <section id="clients" class="clients">
      <div class="container" data-aos="zoom-in">



        <div class="d-flex align-items-center justify-content-center">
          <h1>The Competition</h1>
        </div>


<br></br>

The PHAROS-AIF-MIH Workshop includes a Competition which is split into two Challenges: (i) Multi-Source Covid-19 Detection Challenge; (ii) Fair Disease Diagnosis Challenge.



<br></br>

<br></br>



<h2>How to participate</h2>
 <p> In order to participate, teams will have to register. There is a maximum number of 8 participants in each team. You should follow the below procedure for registration.
 </p>
        
<p> The lead researcher should send  an email from their official address (no personal emails will be accepted) to <a href = "mailto: d.kollias@qmul.ac.uk">d.kollias@qmul.ac.uk</a> with: </p>
<p>&nbsp &nbsp &nbsp i) subject "PHAROS-AIF-MIH Competition: Team Registration"; </p>
<p>&nbsp &nbsp &nbsp ii) <a href = "https://drive.google.com/file/d/1Ut--0MC_hlAjpnT9nwqRPb614cdfoJ5W/view?usp=sharing">this EULA</a>  filled in, signed and attached; </p>  
<p>&nbsp &nbsp &nbsp iii) the lead researcher's official academic/industrial website; the lead researcher cannot be a student (UG/PG/Ph.D.);</p>
<p>&nbsp &nbsp &nbsp iv) the emails of each team member, each one in a separate line in the body of the email; </p>
<p>&nbsp &nbsp &nbsp v) the team's name;</p>
<p>&nbsp &nbsp &nbsp vi) the point of contact name and email address (which member of the team will be the main point of contact for future communications, data access etc) </p>


<p>As a reply, you will receive access to the dataset's images and annotations.</p>

 
 <br></br>



<h2>Competition Contact Information</h2>

 <p> For any queries you may have regarding the Challenges, please contact <a href = "mailto: d.kollias@qmul.ac.uk">d.kollias@qmul.ac.uk</a>.
 </p>
      <br>  


        
<h2>General Information</h2> 
 <p> At the end of the Challenges, each team will have to send us:
 </p>

<p>&nbsp &nbsp &nbsp i) their predictions on the test set, </p>
<p>&nbsp &nbsp &nbsp ii) a link to a Github repository where their solution/source code will be stored, </p>
<p>&nbsp &nbsp &nbsp iii) a link to an ArXiv paper with 2-8 pages describing their proposed methodology, data used and results. </p>

<p>After that, the winner of each Challenge, along with a leaderboard, will be announced.</p>

<p>There will be one winner per Challenge. The top-3 performing teams of each Challenge will have to contribute paper(s) describing their approach, methodology and results to our Workshop; 
  the accepted papers will be part of the CVPR 2026 proceedings. 
  All other teams are also able to submit paper(s) describing their solutions and final results; 
  the accepted papers will be part of the CVPR 2026 proceedings.</p>

 
<p>The Competition's white paper (describing the Competition, the data, the baselines and results) will be ready at a later stage and will be distributed to the participating teams.</p>

 
 <br></br>
 

 <h2>General Rules</h2>

<p>1) Participants can contribute to any of the 2 Challenges.</p>

<p>2) In order to take part in any Challenge, participants will have to register as described above.</p>

<p>4) The winner and the two runner-ups in each Challenge will be asked to also share their trained models so as to check out the validity of the approach.</p>
 <br></br>
 
 <div class="section-title">
          <h2>Competition Important Dates</h2>
        </div>        


<br>  <span> <em>Call for participation announced, team registration begins, data available: </em>  &nbsp &nbsp &nbsp &nbsp &nbsp  January 30, 2026</span> </br>
      
<br>  <span> <em>Test set release: </em> &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp March 9, 2026</span> </br>

<br>  <span> <em>Final submission deadline (Predictions, Code and ArXiv paper): </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 23:59:59 AoE (Anywhere on Earth) March 15, 2026</span> </br>

<br>  <span> <em>Winners Announcement: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp March 17, 2026</span> </br>

              <br>  <span> <em>Final Paper Submission Deadline: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 23:59:59 AoE (Anywhere on Earth) March 18, 2026  </span> </br>

               <br>  <span>  <em>Review decisions sent to authors; Notification of acceptance: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  April 6, 2026 </span> </br> 
              <br>   <span>   <em>Camera ready version: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp April 10, 2026  </span> </br>

 
        

 </div>

</section><!-- End Clients Section -->








        
    <!-- ======= Features Section ======= -->
    <section id="features" class="features">
      <div class="container" data-aos="fade-up">

        

  <div class="d-flex align-items-center justify-content-center">
          <h2>Multi-Source Covid-19 Detection Challenge</h2>
        </div>

<h3>Description</h3> 
CT scans have been collected from four distinct hospitals and medical centers. 
Each scan has been manually annotated to indicate whether it belongs to the Covid-19 or non-Covid-19 category. 
The dataset is divided into training, validation, and test subsets.
Participants will receive the CT scans along with the source identifier for each scan, represented by an ID number from 0 to 3. 
Competing teams are required to develop AI, machine learning, or deep learning models for Covid-19 classification.
Model performance will be evaluated on the test set, which also contains CT scans collected from the same four distinct hospitals and medical centers. 
Model performance will be based on the average macro F1 score achieved across all four sources (hospitals and medical centers), ensuring fair and robust assessment across diverse data distributions.
        

<br> </br>

<h3>Rules</h3> 
The participating teams will be able to use any publicly available datasets as long as they report it in their submitted final paper.
 
<br> </br>
 
<h3>Performance Assessment</h3> 
<p>
The performance measure (P) is the average macro F1 score achieved across all four sources: </p>

<p><small>  <img src="img/aaa2.png" alt="GRNET" class="center" width="262" height="84" > </small></p>

<br>

 
<h3>Baseline Results</h3> 

The baseline network is a CNN-RNN (ResNet-50 + GRU) and its performance on the test set is: </br>
        P = 0.701







      </div>
    </section><!-- End Features Section -->




    
    <!-- ======= Features Section ======= -->
    <section id="features2" class="features">
      <div class="container" data-aos="fade-up">

        

  <div class="d-flex align-items-center justify-content-center">
          <h2>Fair Disease Diagnosis Challenge</h2>
        </div>

<h3>Description</h3> 

The dataset consists of chest CT scans of lung cancer, Covid-19 and healthy subjects. 
Each scan has been annotated to indicate whether it belongs to a healthy subject or a subject diagnosed with Adenocarcinoma, or with Squamous Cell Carcinoma, or with Covid-19.
Each scan contains information on whether the subject is male or female. 
The dataset is divided into training, validation, and test subsets.
Participants will receive the CT scans along with the male/female subject information.
Competing teams are required to develop AI, machine learning, or deep learning models for fair disease diagnosis.
Model performance will be evaluated on the test set and will be based on the average of per-gender macro F1-scores.

   

<br> </br>

<h3>Rules</h3> 
Participating teams may use any publicly available pre-trained models, provided these models have not been specifically pre-trained to classify between healthy individuals and those diagnosed with Adenocarcinoma, Squamous Cell Carcinoma, or Covid-19.
However, all model fine-tuning and methodological development must be conducted exclusively using the dataset provided for the competition.
 
<br> </br>
 
<h3>Performance Assessment</h3> 
<p>
The performance measure (P) is the average of per-gender macro F1-scores.
To calculate this, at first one needs to split the set by gender (Subset A: all male samples; Subset B: all female samples) 
and then compute the macro F1 on each. Therefore, the Final Score is:
</p>

<p><small>  <img src="img/aaa.png" alt="GRNET" class="center" width="312" height="65" > </small></p>
<br>

 
<h3>Baseline Results</h3> 

The baseline network is a CNN-RNN (ResNet-50 + GRU) and its performance on the test set is: </br>
        P = 0.623






      </div>
    </section><!-- End Features Section -->
    




   
    <!-- ======= Features Section ======= -->
    <section id="features3" class="features">
      <div class="container" data-aos="fade-up">

        

  <div class="d-flex align-items-center justify-content-center">
          <h2>Winners</h2>
        </div>

        
<h3>Multi-Source Covid-19 Detection Challenge</h3> 
<p>
There were two winners of this Challenge, teams ACVLAB and FDVTS.
</p>

        <br>

<h3>Fair Disease Diagnosis Challenge</h3> 
The winner of this Challenge is team FDVTS.


      </div>
    </section><!-- End Features Section -->


    



  <div class="d-flex align-items-center justify-content-center">
          <h2>References</h2>
        </div>

         </br>
<p> 
&nbsp If you use the above data, you must cite all following papers and the white paper that will be distributed at a later stage: 
</p>
 

 <ul>
 <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: “Domain adaptation, Explainability & Fairness in AI for Medical Image Analysis: Diagnosis of COVID-19 based on 3-D Chest CT-scans“, CVPR 2024 </p> 
            <p><small>@inproceedings{kollias2024domain,title={Domain adaptation explainability \& fairness in ai for medical image analysis: Diagnosis of covid-19 based on 3-d chest ct-scans},author={Kollias, Dimitrios and Arsenos, Anastasios and Kollias, Stefanos},booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},pages={4907--4914},year={2024}}</small></p>

 <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: “Sam2clip2sam: Vision language model for segmentation of 3d ct scans for covid-19 detection“, EAI ETPHT 2024 </p> 
            <p><small>@article{kollias2024sam2clip2sam,title={Sam2clip2sam: Vision language model for segmentation of 3d ct scans for covid-19 detection},author={Kollias, Dimitrios and Arsenos, Anastasios and Wingate, James and Kollias, Stefanos},journal={arXiv preprint arXiv:2407.15728},year={2024}}</small></p>

<p><i class="ri-check-double-line"></i> D. Gerogiannis, et. al.: “Covid-19 computer-aided diagnosis through ai-assisted ct imaging analysis: Deploying a medical ai system“, IEEE ISBI 2024 </p> 
            <p><small>@inproceedings{gerogiannis2024covid,title={Covid-19 computer-aided diagnosis through ai-assisted ct imaging analysis: Deploying a medical ai system},author={Gerogiannis, Demetris and Arsenos, Anastasios and Kollias, Dimitrios and Nikitopoulos, Dimitris and Kollias, Stefanos},booktitle={2024 IEEE International Symposium on Biomedical Imaging (ISBI)},pages={1--4},year={2024},organization={IEEE}}</small></p>
 
 <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: “AI-Enabled Analysis of 3-D CT Scans for Diagnosis of COVID-19 & its Severity“, IEEE ICASSP 2023 </p> 
            <p><small>@inproceedings{kollias2023ai, title={AI-Enabled Analysis of 3-D CT Scans for Diagnosis of COVID-19 \& its Severity}, author={Kollias, Dimitrios and Arsenos, Anastasios and Kollias, Stefanos}, booktitle={2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)}, pages={1–5}, year={2023}, organization={IEEE}}</small></p>

 <p><i class="ri-check-double-line"></i> A. Arsenos, et. al.: “Data-Driven Covid-19 Detection Through Medical Imaging“, IEEE ICASSP 2023 </p> 
            <p><small>@inproceedings{arsenos2023data, title={Data-Driven Covid-19 Detection Through Medical Imaging}, author={Arsenos, Anastasios and Davidhi, Andjoli and Kollias, Dimitrios and Prassopoulos, Panos and Kollias, Stefanos}, booktitle={2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)}, pages={1–5}, year={2023}, organization={IEEE}}</small></p>

 <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: “A deep neural architecture for harmonizing 3-D input data analysis and decision making in medical imaging“, Neurocomputing 2023 </p> 
            <p><small>@article{kollias2023deep,title={A deep neural architecture for harmonizing 3-D input data analysis and decision making in medical imaging}, author={Kollias, Dimitrios and Arsenos, Anastasios and Kollias, Stefanos}, journal={Neurocomputing}, volume={542}, pages={126244}, year={2023}, publisher={Elsevier}}</small></p>

<p><i class="ri-check-double-line"></i> D. Kollias, et. al.: “AI-MIA: COVID-19 Detection & Severity Analysis through Medical Imaging“, ECCV 2022 </p> 
            <p><small>@inproceedings{kollias2022ai, title={Ai-mia: Covid-19 detection and severity analysis through medical imaging}, author={Kollias, Dimitrios and Arsenos, Anastasios and Kollias, Stefanos}, booktitle={European Conference on Computer Vision}, pages={677–690}, year={2022}, organization={Springer}}</small></p>

<p><i class="ri-check-double-line"></i> A. Arsenos, et. al.: “A Large Imaging Database and Novel Deep Neural Architecture for Covid-19 Diagnosis“, IVMSP 2022 </p> 
            <p><small>@inproceedings{arsenos2022large, title={A Large Imaging Database and Novel Deep Neural Architecture for Covid-19 Diagnosis}, author={Arsenos, Anastasios and Kollias, Dimitrios and Kollias, Stefanos}, booktitle={2022 IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)}, pages={1–5}, year={2022}, organization={IEEE} }</small></p>

<p><i class="ri-check-double-line"></i> D. Kollias, et. al.: “MIA-COV19D: COVID-19 Detection through 3-D Chest CT Image Analysis“, ICCV 2021 </p> 
            <p><small>@inproceedings{kollias2021mia, title={Mia-cov19d: Covid-19 detection through 3-d chest ct image analysis}, author={Kollias, Dimitrios and Arsenos, Anastasios and Soukissian, Levon and Kollias, Stefanos}, booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages={537–544}, year={2021} }</small></p>

<p><i class="ri-check-double-line"></i> D. Kollias, et. al.: “Deep transparent prediction through latent representation analysis”, 2020 </p> 
            <p><small>@article{kollias2020deep, title={Deep transparent prediction through latent representation analysis}, author={Kollias, Dimitrios and Bouas, N and Vlaxos, Y and Brillakis, V and Seferis, M and Kollia, Ilianna and Sukissian, Levon and Wingate, James and Kollias, S}, journal={arXiv preprint arXiv:2009.07044}, year={2020}}</small></p>

<p><i class="ri-check-double-line"></i> D. Kollias, et. al.: “Transparent Adaptation in Deep Medical Image Diagnosis”, TAILOR 2020 </p> 
            <p><small>@inproceedings{kollias2020transparent, title={Transparent Adaptation in Deep Medical Image Diagnosis.}, author={Kollias, Dimitris and Vlaxos, Y and Seferis, M and Kollia, Ilianna and Sukissian, Levon and Wingate, James and Kollias, Stefanos D}, booktitle={TAILOR}, pages={251–267}, year={2020}}</small></p>

   <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: “Deep neural architectures for prediction in healthcare”, CIS 2018 </p> 
            <p><small>@article{kollias2018deep, title={Deep neural architectures for prediction in healthcare}, author={Kollias, Dimitrios and Tagaris, Athanasios and Stafylopatis, Andreas and Kollias, Stefanos and Tagaris, Georgios}, journal={Complex \& Intelligent Systems}, volume={4}, number={2}, pages={119–131}, year={2018}, publisher={Springer}}</small></p>







    <!-- ======= Cta Section ======= -->
    <section id="testimonials" class="testimonials">
      <div class="container" data-aos="zoom-in">


  <div class="d-flex align-items-center justify-content-center">
          <h2>Sponsors</h2>
        </div>

         </br>
<p> 
The PHAROS-AIF-MIH Workshop has been generously supported by:
 
</p>

         <ul>

           <p><i class="ri-check-double-line"></i>  GRNET – National Infrastructures for Research and Technology </p>
<p><small>  <img src="img/GRNET.png" alt="GRNET" class="center" width="320" height="138" > </small></p>
           
           <p><i class="ri-check-double-line"></i>  Queen Mary University of London </p>
<p><small>  <img src="img/qmul.jpeg" alt="QMUL"> </small></p>

<p><i class="ri-check-double-line"></i>  Digital Environment Research Institute </p>
<p><small>  <img src="img/DERI.gif" alt="DERI" class="center" width="600" height="200"> </small></p>
      
        
<p><i class="ri-check-double-line"></i>  Institute of Communication and Computer Systems  </p>
<p><small>  <img src="img/ICCS.webp" alt="ICCS"> </small></p>

        

        



</ul>


 


 


 



 



   </div>
    </section><!-- End Counts Section -->









        







<!-- Comments go here 


          <div class="col-lg-4 col-md-6 d-flex align-items-stretch mt-4" data-aos="zoom-in" data-aos-delay="300">
            <div class="icon-box">
              <div class="icon"><i class="bx bx-arch"></i></div>
              <h4><a href="">Divera don</a></h4>
              <p>Modi nostrum vel laborum. Porro fugit error sit minus sapiente sit aspernatur</p>
            </div>
          </div>

        </div>

      </div>
    </section>








    <section id="testimonials" class="testimonials">
      <div class="container" data-aos="zoom-in">

        <div class="testimonials-slider swiper" data-aos="fade-up" data-aos-delay="100">
          <div class="swiper-wrapper">

            <div class="swiper-slide">
              <div class="testimonial-item">
                <img src="img/testimonials/testimonials-1.jpg" class="testimonial-img" alt="">
                <h3>Saul Goodman</h3>
                <h4>Ceo &amp; Founder</h4>
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  Proin iaculis purus consequat sem cure digni ssim donec porttitora entum suscipit rhoncus. Accusantium quam, ultricies eget id, aliquam eget nibh et. Maecen aliquam, risus at semper.
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div>

            <div class="swiper-slide">
              <div class="testimonial-item">
                <img src="img/testimonials/testimonials-2.jpg" class="testimonial-img" alt="">
                <h3>Sara Wilsson</h3>
                <h4>Designer</h4>
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  Export tempor illum tamen malis malis eram quae irure esse labore quem cillum quid cillum eram malis quorum velit fore eram velit sunt aliqua noster fugiat irure amet legam anim culpa.
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div>

            <div class="swiper-slide">
              <div class="testimonial-item">
                <img src="img/testimonials/testimonials-3.jpg" class="testimonial-img" alt="">
                <h3>Jena Karlis</h3>
                <h4>Store Owner</h4>
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  Enim nisi quem export duis labore cillum quae magna enim sint quorum nulla quem veniam duis minim tempor labore quem eram duis noster aute amet eram fore quis sint minim.
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div>

            <div class="swiper-slide">
              <div class="testimonial-item">
                <img src="img/testimonials/testimonials-4.jpg" class="testimonial-img" alt="">
                <h3>Matt Brandon</h3>
                <h4>Freelancer</h4>
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  Fugiat enim eram quae cillum dolore dolor amet nulla culpa multos export minim fugiat minim velit minim dolor enim duis veniam ipsum anim magna sunt elit fore quem dolore labore illum veniam.
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div>

            <div class="swiper-slide">
              <div class="testimonial-item">
                <img src="img/testimonials/testimonials-5.jpg" class="testimonial-img" alt="">
                <h3>John Larson</h3>
                <h4>Entrepreneur</h4>
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  Quis quorum aliqua sint quem legam fore sunt eram irure aliqua veniam tempor noster veniam enim culpa labore duis sunt culpa nulla illum cillum fugiat legam esse veniam culpa fore nisi cillum quid.
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div>
          </div>
          <div class="swiper-pagination"></div>
        </div>

      </div>
    </section>



    <section id="contact" class="contact">
      <div class="container" data-aos="fade-up">

        <div class="section-title">
          <h2>Contact</h2>
          <p>Contact Us</p>
        </div>

        <div>
          <iframe style="border:0; width: 100%; height: 270px;" src="https://www.google.com/maps/embed?pb=!1m14!1m8!1m3!1d12097.433213460943!2d-74.0062269!3d40.7101282!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x0%3A0xb89d1fe6bc499443!2sDowntown+Conference+Center!5e0!3m2!1smk!2sbg!4v1539943755621" frameborder="0" allowfullscreen></iframe>
        </div>

        <div class="row mt-5">

          <div class="col-lg-4">
            <div class="info">
              <div class="address">
                <i class="bi bi-geo-alt"></i>
                <h4>Location:</h4>
                <p>A108 Adam Street, New York, NY 535022</p>
              </div>

              <div class="email">
                <i class="bi bi-envelope"></i>
                <h4>Email:</h4>
                <p>info@example.com</p>
              </div>

              <div class="phone">
                <i class="bi bi-phone"></i>
                <h4>Call:</h4>
                <p>+1 5589 55488 55s</p>
              </div>

            </div>

          </div>

          <div class="col-lg-8 mt-5 mt-lg-0">

            <form action="forms/contact.php" method="post" role="form" class="php-email-form">
              <div class="row">
                <div class="col-md-6 form-group">
                  <input type="text" name="name" class="form-control" id="name" placeholder="Your Name" required>
                </div>
                <div class="col-md-6 form-group mt-3 mt-md-0">
                  <input type="email" class="form-control" name="email" id="email" placeholder="Your Email" required>
                </div>
              </div>
              <div class="form-group mt-3">
                <input type="text" class="form-control" name="subject" id="subject" placeholder="Subject" required>
              </div>
              <div class="form-group mt-3">
                <textarea class="form-control" name="message" rows="5" placeholder="Message" required></textarea>
              </div>
              <div class="my-3">
                <div class="loading">Loading</div>
                <div class="error-message"></div>
                <div class="sent-message">Your message has been sent. Thank you!</div>
              </div>
              <div class="text-center"><button type="submit">Send Message</button></div>
            </form>

          </div>

        </div>

      </div>
    </section>
-->



  </main><!-- End #main -->


  <footer id="footer">
    <div class="footer-top">
      <div class="container">
        <div class="row">

   <div class="container">
      <div class="copyright">
        &copy; Copyright <strong><span>Gp & Dimitrios Kollias</span></strong>. All Rights Reserved
      </div>
      <div class="credits">
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
  </footer>
  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

        </div>
      </div>
    </div>

 
  <!-- Vendor JS Files -->
  <script src="vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="vendor/aos/aos.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="vendor/glightbox/js/glightbox.min.js"></script>
  <script src="vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="vendor/swiper/swiper-bundle.min.js"></script>
  <script src="vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="js/main.js"></script>

</body>

</html>














